{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_04_Pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPGPuZjC/VApzeVzYX/bsHx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VitalyGladyshev/gb_pytorch/blob/main/HW_04_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fJ_QYJ6-EnK"
      },
      "source": [
        "#ДЗ 4 Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i63dy6pn-JzA"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUZPcTfDLpsD"
      },
      "source": [
        "Необходимо доработать обучение нейросети, что мы разбирали на уроке.(Посмотрите чего не хватает в процессе обучения и подготовки данных)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUKnP4a-MzSR"
      },
      "source": [
        "### Базовый вариант"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elUZPPUJ9co8",
        "outputId": "e23bec5e-4831-4b13-9128-68e324c024b8"
      },
      "source": [
        "pip install segmentation_models_pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting segmentation_models_pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/54/8953f9f7ee9d451b0f3be8d635aa3a654579abf898d17502a090efe1155a/segmentation_models_pytorch-0.1.3-py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.8MB/s \n",
            "\u001b[?25hCollecting pretrainedmodels==0.7.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[?25hCollecting timm==0.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/2d/39ecc56fbb202e1891c317e8e44667299bc3b0762ea2ed6aaaa2c2f6613c/timm-0.3.2-py3-none-any.whl (244kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 8.8MB/s \n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/cb/0309a6e3d404862ae4bc017f89645cf150ac94c14c88ef81d215c8e52925/efficientnet_pytorch-0.6.3.tar.gz\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from segmentation_models_pytorch) (0.8.1+cu101)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.7.0+cu101)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.41.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (1.18.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n",
            "Building wheels for collected packages: pretrainedmodels, efficientnet-pytorch\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60963 sha256=8e239dfed85d3a0a67e39a7bc8e92d6866615c4631e3fe7bd00b92aee91c21a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-cp36-none-any.whl size=12420 sha256=b2b9330abf39bf846de8502cd529dca52a62565b85fcf68be731e057f720a55d\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/1e/a9/2a578ba9ad04e776e80bf0f70d8a7f4c29ec0718b92d8f6ccd\n",
            "Successfully built pretrainedmodels efficientnet-pytorch\n",
            "Installing collected packages: munch, pretrainedmodels, timm, efficientnet-pytorch, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.3 timm-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bonov13NCly"
      },
      "source": [
        "import segmentation_models_pytorch as smp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9JbwZbBNCo9"
      },
      "source": [
        "!pip install -U -q kaggle\r\n",
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "v1m5nL9oWx3D",
        "outputId": "34e5ee1e-43c9-4ef9-aacd-e9fad4b6fa2f"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c421ebad-c615-49d8-8098-96f1bb1848a9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c421ebad-c615-49d8-8098-96f1bb1848a9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"vitalygladyshev\",\"key\":\"3353367a1e22b48542ed3c3946bdf3e4\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCJ6ERzEWx59",
        "outputId": "54cb3cd4-7946-46d8-9078-165bec975892"
      },
      "source": [
        "!mv kaggle.json ~/.kaggle/\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json\r\n",
        "!ls -la ~/.kaggle/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 2 root root 4096 Dec 15 07:45 .\n",
            "drwx------ 1 root root 4096 Dec 15 07:44 ..\n",
            "-rw------- 1 root root   71 Dec 15 07:44 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMrctSGWyAW",
        "outputId": "791d5f10-57df-403a-91a9-0fd5f7c817c6"
      },
      "source": [
        "!kaggle datasets list -s lyft-udacity-challenge"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "ref                                                       title                                               size  lastUpdated          downloadCount  \n",
            "--------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  \n",
            "kumaresanmanickavelu/lyft-udacity-challenge               Semantic Segmentation for Self Driving Cars          5GB  2018-05-18 05:59:42           1996  \n",
            "morrisb/semantic-segmentation-with-carla-and-tpus         Semantic Segmentation With CARLA And TPUs            3GB  2020-10-18 10:11:51             16  \n",
            "austenmy/udacity-enrollments                              Udacity Enrollments                                 10KB  2018-04-06 00:54:43             91  \n",
            "pbdanny/udacity-project-submissions                       Udacity Project Submission                          24KB  2018-05-10 11:41:36             45  \n",
            "samtyagi/audacity-ab-testing                              audacity ab testing                                103KB  2019-01-29 15:00:02            114  \n",
            "souravs17031999/flowerclassifierudacitypretrainedweights  Flower-classifier-Udacity-pretrained-Weights       211MB  2020-04-19 12:22:52             24  \n",
            "austenmy/udacity-daily-engagement                         Udacity Daily Engagement                           649KB  2018-04-06 01:01:01            100  \n",
            "luizoamorim/analysis-bay-area-bike-share-udacity          Analysis Bay Area Bike Share Udacity               243KB  2017-11-10 00:16:53            108  \n",
            "fabiocorreacordeiro/udacity-titanic-data                  Udacity Titanic Data                                22KB  2017-11-01 20:46:37             44  \n",
            "tammyrotem/udacity-ab-testing-finalproject-data           Udacity_AB_Testing_FinalProject_Data                679B  2018-01-03 14:55:38             60  \n",
            "schirmerchad/bostonhoustingmlnd                           Boston Housing                                       4KB  2017-06-11 15:07:11           8777  \n",
            "madushan1996/udacity                                      Udacity                                            420MB  2020-08-23 15:21:22              0  \n",
            "abhinavmanoj/udacity-simulator-dataset                    Udacity Simulator Dataset                           32MB  2020-06-09 18:30:13              7  \n",
            "jpdaza/udacity-intro-to-data-analysis                     Udacity Intro to Data Analysis                       9MB  2018-06-10 18:10:56             70  \n",
            "tusharcode/selfdriving-car-udacity                        Self-Driving Car Udacity                           243MB  2020-04-23 03:44:26             23  \n",
            "evertonsa/udacitybrazilmedicalappointments                UdacityBrazilMedicalAppointments                     2MB  2017-10-29 15:55:19             18  \n",
            "zaynena/selfdriving-car-simulator                         Self-Driving Car Simulator                           2GB  2019-04-06 23:38:58            511  \n",
            "priya2908/chopsticks-1992                                 Beginner Projects - Ergonomic Study on Chopsticks    1KB  2017-06-11 19:44:20            371  \n",
            "swimmingwhale/udacitymachinelearningdatasets              Udacity-Machine-Learning-Datasets                    4MB  2018-09-08 01:45:03              2  \n",
            "davydev/udacity-intro-to-ml                               udacity-intro-to-machine-learning                    4MB  2020-07-26 08:42:09              0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HplVxzolWx9U",
        "outputId": "ab93cc8d-fd0f-4852-e24e-94e59e1762d1"
      },
      "source": [
        "!kaggle datasets download -d kumaresanmanickavelu/lyft-udacity-challenge\r\n",
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading lyft-udacity-challenge.zip to /content\n",
            "100% 5.09G/5.11G [01:12<00:00, 95.1MB/s]\n",
            "100% 5.11G/5.11G [01:12<00:00, 75.9MB/s]\n",
            "lyft-udacity-challenge.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96oaWxn-Z7DE"
      },
      "source": [
        "from zipfile import ZipFile"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq_Y3X0DaCqZ"
      },
      "source": [
        "zip_file = ZipFile('lyft-udacity-challenge.zip')\r\n",
        "zip_file.extractall(\"./lyft-udacity-challenge\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHnzwQBIaCyq",
        "outputId": "d2667abe-9f9a-4d26-deba-645c7254e798"
      },
      "source": [
        "!path = \"lyft-udacity-challenge\"\r\n",
        "!ls -ls $path"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: path: command not found\n",
            "total 40\n",
            "4 drwxr-xr-x 3 root root 4096 Dec 15 08:09 dataa\n",
            "4 drwxr-xr-x 3 root root 4096 Dec 15 08:08 dataA\n",
            "4 drwxr-xr-x 3 root root 4096 Dec 15 08:09 datab\n",
            "4 drwxr-xr-x 3 root root 4096 Dec 15 08:08 dataB\n",
            "4 drwxr-xr-x 3 root root 4096 Dec 15 08:09 datac\n",
            "4 drwxr-xr-x 3 root root 4096 Dec 15 08:08 dataC\n",
            "4 drwxr-xr-x 3 root root 4096 Dec 15 08:09 datad\n",
            "4 drwxr-xr-x 3 root root 4096 Dec 15 08:08 dataD\n",
            "4 drwxr-xr-x 3 root root 4096 Dec 15 08:09 datae\n",
            "4 drwxr-xr-x 3 root root 4096 Dec 15 08:08 dataE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEyCR3kiWyDl"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiIr9JOpd14M"
      },
      "source": [
        "labels = ['Unlabeled','Building','Fence','Other',\r\n",
        "          'Pedestrian', 'Pole', 'Roadline', 'Road',\r\n",
        "          'Sidewalk', 'Vegetation', 'Car','Wall',\r\n",
        "          'Traffic sign']"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1x7H1h1edCr"
      },
      "source": [
        "path = \"lyft-udacity-challenge\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCBsoCsed18L"
      },
      "source": [
        "cameraRGB = []\r\n",
        "cameraSeg = []\r\n",
        "for root, dirs, files in os.walk(path):\r\n",
        "    for name in files:\r\n",
        "        f = os.path.join(root, name)\r\n",
        "        if 'CameraRGB' in f:\r\n",
        "            cameraRGB.append(f)\r\n",
        "        elif 'CameraSeg' in f:\r\n",
        "            cameraSeg.append(f)\r\n",
        "        else:\r\n",
        "            break"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk3ZwXvDd2AU",
        "outputId": "c9eae198-5123-46fd-aa94-285ae13408a0"
      },
      "source": [
        "cameraRGB[-1], cameraSeg[-1]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lyft-udacity-challenge/datae/dataE/CameraRGB/06_00_079.png',\n",
              " 'lyft-udacity-challenge/datae/dataE/CameraSeg/06_00_079.png')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU1-185NkF81",
        "outputId": "5d39b025-1852-4e7c-f5ab-42a08e1c1b31"
      },
      "source": [
        "df = pd.DataFrame({'cameraRGB': cameraRGB, 'cameraSeg': cameraSeg})\r\n",
        "# Отсортируем  датафрейм по значениям\r\n",
        "df.sort_values(by='cameraRGB',inplace=True)\r\n",
        "# Используем функцию,\r\n",
        "# лагодаря которой индексация значений \r\n",
        "# будет начинаться с 0.\r\n",
        "df.reset_index(drop=True, inplace=True)\r\n",
        "# Выведем первые ять значений нашего датафрейма\r\n",
        "print(df.head(5))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                           cameraRGB                                          cameraSeg\n",
            "0  lyft-udacity-challenge/dataA/dataA/CameraRGB/0...  lyft-udacity-challenge/dataA/dataA/CameraSeg/0...\n",
            "1  lyft-udacity-challenge/dataA/dataA/CameraRGB/0...  lyft-udacity-challenge/dataA/dataA/CameraSeg/0...\n",
            "2  lyft-udacity-challenge/dataA/dataA/CameraRGB/0...  lyft-udacity-challenge/dataA/dataA/CameraSeg/0...\n",
            "3  lyft-udacity-challenge/dataA/dataA/CameraRGB/0...  lyft-udacity-challenge/dataA/dataA/CameraSeg/0...\n",
            "4  lyft-udacity-challenge/dataA/dataA/CameraRGB/0...  lyft-udacity-challenge/dataA/dataA/CameraSeg/0...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G6-6RCtkGGC",
        "outputId": "047a597d-7906-47c1-a55c-102eaa4746b2"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   cameraRGB  10000 non-null  object\n",
            " 1   cameraSeg  10000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 156.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-tm-DhHd2D9"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from PIL import Image\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import functional as F\r\n",
        "import time"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "molwhPU5kGQE"
      },
      "source": [
        "class CustomDatasetFromImages(Dataset):\r\n",
        "    def __init__(self, data_info):\r\n",
        "        # Подаем наш подготовленный датафрейм\r\n",
        "        self.data_info = data_info\r\n",
        "        \r\n",
        "        # Разделяем датафрейм на rgb картинки \r\n",
        "        self.image_arr = self.data_info.iloc[:,0]\r\n",
        "        # и на сегментированные картинки\r\n",
        "        self.label_arr = self.data_info.iloc[:,1]\r\n",
        "        \r\n",
        "        # Количество пар картинка-сегментация\r\n",
        "        self.data_len = len(self.data_info.index)\r\n",
        "    def __getitem__(self, index):\r\n",
        "        # Читаем картинку и сразу же представляем ее в виде numpy-массива \r\n",
        "        # размера 600х800 float-значний\r\n",
        "        img = np.asarray(Image.open(self.image_arr[index])).astype('float')\r\n",
        "        # Нормализуем изображение в значениях [0,1]\r\n",
        "        img = torch.as_tensor(img)/255    \r\n",
        "        # 1) unsqueeze - меняет размерность img c (600, 800, 3) -> (1, 600, 800, 3),\r\n",
        "        # т.е. оборачивает картинку в батч размером в одну картинку\r\n",
        "        # 2) permute - меняет местами измерения , т.е. (1, 600, 800, 3) -> (1, 3, 600, 800)\r\n",
        "        img = img.unsqueeze(0).permute(0,3,1,2)\r\n",
        "        # Мы используем функцию интерполяции для того,\r\n",
        "        # чтобы поменять рамерность картинки с 800х600 на 256х256\r\n",
        "        img = F.interpolate(input=img, size=256, align_corners=False, mode='bicubic')\r\n",
        "        \r\n",
        "        # итаем сегментированную картинку и сразу же представляем ее в виде numpy-массива \r\n",
        "        # размера 600х800 float-значний\r\n",
        "        lab = np.asarray(plt.imread(self.label_arr[index]))[:,:,0]*255\r\n",
        "        \r\n",
        "        # Упаковываем ее в pytorch-тензор и оборачиваем ее в батч из одной каринки,\r\n",
        "        # но при этом заполняем 13 каналов масками нужных классов\r\n",
        "        # Т.е. там, где например класс автомобилей (10 по счету канал) - все пиксели 0 \r\n",
        "        # если не принадлежат классу, и 1 если принадлежат \r\n",
        "        x_out = torch.as_tensor(np.where(lab == 0, 255, 0)).unsqueeze(0)\r\n",
        "        for i in range(1, 13):\r\n",
        "            mask = np.asarray(plt.imread(self.label_arr[index]))[:,:,0]*255\r\n",
        "            mask = np.where(mask == i, 255, 0)\r\n",
        "            x = torch.as_tensor(mask).unsqueeze(0)\r\n",
        "            x_out =  torch.cat((x_out,x),dim=0)    \r\n",
        "        x_out = x_out.float()\r\n",
        "        \r\n",
        "        lab = x_out.unsqueeze(0)\r\n",
        "        # делаем ресайз картинки на 256х256\r\n",
        "        lab = F.interpolate(input=lab, size=256, mode='nearest')\r\n",
        "        \r\n",
        "        \r\n",
        "        \r\n",
        "        return (img.float(), lab.float())\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.data_len"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv78_E2PlQOs"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# 80 % в тренировочную выборку, 20 - в тестовую\r\n",
        "X_data, X_test = train_test_split(df, test_size=0.2)\r\n",
        "# 20 - в валидационную\r\n",
        "X_train, X_valid = train_test_split(X_data, test_size=0.2)\r\n",
        "\r\n",
        "# Упорядочиваем индексацию\r\n",
        "X_train.reset_index(drop=True,inplace=True)\r\n",
        "X_valid.reset_index(drop=True,inplace=True)\r\n",
        "X_test.reset_index(drop=True,inplace=True)\r\n",
        "\r\n",
        "# Оборачиваем каждую выборку в наш кастомный датасет\r\n",
        "train_data = CustomDatasetFromImages(X_train)\r\n",
        "valid_data = CustomDatasetFromImages(X_valid)\r\n",
        "test_data = CustomDatasetFromImages(X_test)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Eln4fgtmirG"
      },
      "source": [
        "train_data_loader = DataLoader(train_data, batch_size=1, shuffle=True)\r\n",
        "valid_data_loader = DataLoader(valid_data, batch_size=1, shuffle=True)\r\n",
        "test_data_loader = DataLoader(test_data, batch_size=1, shuffle=False)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7ittFGJd2IL",
        "outputId": "12cc2dde-a8f3-4955-bf05-a483d770bef4"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "print(device)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lggxKFWjoLuH"
      },
      "source": [
        "class SoftDiceLoss(nn.Module):\r\n",
        "    def __init__(self, weight=None, size_average=True):\r\n",
        "        super(SoftDiceLoss, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, logits, targets):\r\n",
        "        smooth =1\r\n",
        "        num = targets.size(0)\r\n",
        "        probs = F.sigmoid(logits)\r\n",
        "        m1 = probs.view(num, -1)\r\n",
        "        m2 = targets.view(num, -1)\r\n",
        "        intersection = (m1 * m2)\r\n",
        "\r\n",
        "        score =2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\r\n",
        "        score =1 - score.sum() / num\r\n",
        "        return score"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x04twoGm3xk"
      },
      "source": [
        "learning_rate = 0.001\r\n",
        "epochs = 5"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH4t11-ipT4X"
      },
      "source": [
        "# создание модели\r\n",
        "segmodel = smp.Unet()\r\n",
        "segmodel = smp.Unet('resnet34', classes=13, activation='softmax').to(device)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WbxUle-pUA7"
      },
      "source": [
        "criterion = SoftDiceLoss()\r\n",
        "optimizer = torch.optim.Adam(segmodel.parameters(), lr=1e-10)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "6HSeQtHQMxjr",
        "outputId": "04dc322a-704b-4b89-ae7f-6bfc16b507f7"
      },
      "source": [
        "epoch_losses = []\r\n",
        "\r\n",
        "for epoch in range(20):  # loop over the dataset multiple times\r\n",
        "    running_loss = 0.0\r\n",
        "    epoch_loss = []\r\n",
        "    t = 0\r\n",
        "    time1 = time.time()\r\n",
        "    for i, data in enumerate(train_data_loader, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, labels = data\r\n",
        "        inputs = inputs.to(device) # .cuda()\r\n",
        "        labels = labels.to(device) # .cuda()\r\n",
        "\r\n",
        "\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = segmodel(inputs[0])\r\n",
        "        loss = criterion(outputs, labels[0,0,:,:,:])\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        epoch_loss.append(loss.item())\r\n",
        "        t+=1\r\n",
        "        if not (t+1) % 1000:    # print every 1000 mini-batches\r\n",
        "            print(f'Epoch: {epoch}, batchcount: {t}, avg. loss for last 1000 images: {running_loss/1000}')\r\n",
        "    time2 = time.time()\r\n",
        "    print(f'Epoch {epoch+1}, loss: ',np.mean(epoch_loss),f' time = {time2-time1} sec')\r\n",
        "    epoch_losses.append(epoch_loss)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/segmentation_models_pytorch/base/modules.py:102: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.activation(x)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-6492c3adbb46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZoLR8OgptSM"
      },
      "source": [
        "### Чего-то не хватает? Добавим PyTorch огня! :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuQzqEagMjFJ"
      },
      "source": [
        "pip install git+https://github.com/pytorch/ignite"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}