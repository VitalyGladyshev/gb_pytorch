{"nbformat":4,"nbformat_minor":4,"metadata":{"notebookId":"c2b53adc-5e66-4c3c-a38c-4911243da2c5","language_info":{"file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"}},"cells":[{"cell_type":"markdown","source":"# ДЗ 2","metadata":{"cellId":"veg7amh5kk8cfeiiswknk"}},{"cell_type":"code","source":"from cloud_ml.storage.api import Storage\n\n# To retrieve client secret:\n# 1. Go to link: https://developers.google.com/drive/api/v3/quickstart/python\n# 2. Press \"Enable the drive API\"\n# 3. Choose \"TVs and limited input devices\"\nclient_secret = {8:\")\"}\n\n# downloading contents of the remote file into the local one\ngdrive = Storage.gdrive(client_secret)\ngdrive_file_id = '1CUZnBtYwifVXS21yVg62T-vrPVayso5H'\ndst_path = 'data/nturgbd_skeletons_s001_to_s017.zip'\ngdrive.get(gdrive_file_id, dst_path)","metadata":{"cellId":"3isyc7267f7bkdgm7w7vud","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"                                                                                                            \r"}],"execution_count":10},{"cell_type":"code","source":"from zipfile import ZipFile","metadata":{"cellId":"jvfl6qx2i8rjgunh0i4lxb","trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# fname = 'datasets.zip'\nzipfile = ZipFile(dst_path)\nzipfile.extractall()","metadata":{"cellId":"x6uvzuh1m5m0om1gr2ml4bc","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/kernel/lib/python3.7/site-packages/ml_kernel/kernel.py:475: UserWarning: The following variables cannot be serialized: zipfile\n  warnings.warn(message)\n"}],"execution_count":12},{"cell_type":"code","source":"client_secret = {\"installed\":{\"client_id\":\"857370663792-fiev084kokkeivk2qbvq95qvc9gcs6h0.apps.googleusercontent.com\",\"project_id\":\"quickstart-1606583721372\",\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_secret\":\"DKtGi6FZOWvWrPhr5Irp-Wbm\",\"redirect_uris\":[\"urn:ietf:wg:oauth:2.0:oob\",\"http://localhost\"]}}\ngdrive = Storage.gdrive(client_secret)\n\ngdrive_file_ms_id = '1eCKZ_4MEGHE3IK0t-tS-AOF3D4lCXkdN'\nfile_path_sampl_miss = 'NTU_RGBD_samples_with_missing_skeletons.txt'\ngdrive.get(gdrive_file_ms_id, file_path_sampl_miss)","metadata":{"cellId":"jjgwfotc3vknftcb58n9cf","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"                                                                                                \r"}],"execution_count":17},{"cell_type":"markdown","source":"## Задание 1","metadata":{"cellId":"k5ac8zote8n3dr0qi0u27g"}},{"cell_type":"markdown","source":"Собрать класс датасет с функциями из ноутбука по генерации датасета. Надо функции перенести в класс + добавить возможность при инициализации менять длину последовательности кадров.","metadata":{"cellId":"zprgeqeqjhoyueyadshmpo"}},{"cell_type":"code","source":"#!L\nimport os\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch import nn\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset","metadata":{"cellId":"2pal488j0mffchi620uyzb","trusted":true},"outputs":[],"execution_count":118},{"cell_type":"code","source":"#!L\nclass Croped_Raw_Data:\n    def __init__(self, data_path, broken_files_path):\n        subjects = list(range(0, 28)) #количество людей выполняющих действия\n        classes = [8, 10, 22, 23, 27, 21] #классы которые будем использовать для обучения, полный список прдставлен тут https://github.com/shahroudy/NTURGB-D\n        cameras = [1, 2, 3] \n        \n        labels = []\n        counter = 0\n        files_counter = {}\n        \n        self.__files = []\n        self.__action_classes = {}\n\n        with open(broken_files_path, 'r') as f:\n            broken_files = f.read().split(\"\\n\")\n\n        raw_files = os.listdir(data_path)\n\n        for filename in raw_files:\n            if filename not in broken_files:\n                action_class = int(filename[filename.find('A') + 1:filename.find('A') + 4])\n                subject_id = int(filename[filename.find('P') + 1:filename.find('P') + 4])\n                camera_id = int(filename[filename.find('C') + 1:filename.find('C') + 4])\n                if action_class in classes and camera_id in cameras:  #and subject_id in subjects:\n                    if action_class in self.__action_classes:\n                        if files_counter[action_class] < 120:\n                            self.__files.append([filename, self.__action_classes[action_class]])\n                            files_counter[action_class] = files_counter[action_class] + 1\n                    else:\n                        self.__action_classes.update({action_class : counter})\n                        files_counter.update({action_class : 1})\n                        counter += 1\n                        self.__files.append([filename, self.__action_classes[action_class]])\n                        \n    \n    def get_files(self):\n        return self.__files\n    \n    \n    def get_classes(self):\n        return self.__action_classes","metadata":{"cellId":"pyhw249fd1bl2e4zcq5a","trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#!L\ndata_path = \"nturgb+d_skeletons/\"\nbroken_files_path = \"NTU_RGBD_samples_with_missing_skeletons.txt\"\n\ndata = Croped_Raw_Data(data_path, broken_files_path)","metadata":{"cellId":"izah7qx57cqfrade1d6ap","trusted":true},"outputs":[],"execution_count":96},{"cell_type":"code","source":"#!L\ndata.get_classes()","metadata":{"cellId":"wvo1ndfkvvk4pbzphlqjad","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"{22: 0, 23: 1, 8: 2, 27: 3, 10: 4, 21: 5}"},"metadata":{}}],"execution_count":97},{"cell_type":"code","source":"#!L\na, b = torch.utils.data.random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\nprint(a)","metadata":{"cellId":"caiiaig26trmz3l4kvxlxn","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"<torch.utils.data.dataset.Subset object at 0x7fb9b1954290>\n"}],"execution_count":57},{"cell_type":"code","source":"#!L\nclass Skeleton_Dataset(Dataset):\n    def __init__(self, \n                 inc_data, \n                 data_path, \n                 transform=None, \n                 chonk_len=45):\n        \n        self.transform = transform\n        self.chonk_len = chonk_len\n        self.data_path = data_path\n\n        data_lst = []\n        labels_lst = []\n        numbers = {v: 0 for k, v in inc_data.get_classes().items()}\n\n        for file in inc_data.get_files():\n            frames_blocks, label = self.create_coords_blocks(file)\n            if label != [] and numbers[label[0]] <= 150:\n                numbers[label[0]] = numbers[label[0]] + len(label)\n                data_lst = data_lst + frames_blocks\n                labels_lst = labels_lst + label\n        data_np = np.asarray(data_lst)\n        labels_np = np.asarray(labels_lst)\n\n        data_sq = data_np.reshape(len(data_np), -1)\n        self.data = pd.DataFrame(data_sq)\n        self.data['labels'] = labels_np\n        self.labels = self.data.iloc[:,-1]\n\n\n    def __len__(self):\n        return len(self.data) \n    \n    \n    def __getitem__(self, idx):\n        item = np.asarray(self.data.iloc[idx,:-1]).reshape(self.chonk_len, 25*3)\n        label = self.labels.values[idx]\n        if self.transform != None:\n            item = self.transform(item)\n        return (item, label)\n    \n    \n    def read_skeleton_filter(self, file):\n        with open(file, 'r') as f:\n            skeleton_sequence = {}\n            skeleton_sequence['numFrame'] = int(f.readline())\n            skeleton_sequence['frameInfo'] = []\n            for t in range(skeleton_sequence['numFrame']):\n                frame_info = {}\n                frame_info['numBody'] = int(f.readline())\n                frame_info['bodyInfo'] = []\n\n                for m in range(frame_info['numBody']):\n                    body_info = {}\n                    body_info_key = [\n                        'bodyID', 'clipedEdges', 'handLeftConfidence',\n                        'handLeftState', 'handRightConfidence', 'handRightState',\n                        'isResticted', 'leanX', 'leanY', 'trackingState'\n                    ]\n                    body_info = {\n                        k: float(v)\n                        for k, v in zip(body_info_key, f.readline().split())\n                    }\n                    body_info['numJoint'] = int(f.readline())\n                    body_info['jointInfo'] = []\n                    for v in range(body_info['numJoint']):\n                        joint_info_key = [\n                            'x', 'y', 'z', 'depthX', 'depthY', 'colorX', 'colorY',\n                            'orientationW', 'orientationX', 'orientationY',\n                            'orientationZ', 'trackingState'\n                        ]\n                        joint_info = {\n                            k: float(v)\n                            for k, v in zip(joint_info_key, f.readline().split())\n                        }\n                        body_info['jointInfo'].append(joint_info)\n                    frame_info['bodyInfo'].append(body_info)\n                skeleton_sequence['frameInfo'].append(frame_info)\n\n        return skeleton_sequence\n\n\n    def read_xyz(self, file, max_body=1, num_joint=25):\n        seq_info = self.read_skeleton_filter(file)\n        data = np.zeros((max_body, seq_info['numFrame'], num_joint, 3))\n        for n, f in enumerate(seq_info['frameInfo']):\n            for m, b in enumerate(f['bodyInfo']):\n                for j, v in enumerate(b['jointInfo']):\n                    if m < max_body and j < num_joint:\n                        data[m, n, j, :] = [v['x'], v['y'], v['z']]\n        return data\n\n\n    def create_coords_blocks(self, test_file):   \n        frame_counter = 0\n        new_labels = []\n        new_frames = []\n        blocks = []\n\n        test_frames = self.read_xyz(self.data_path + test_file[0])[0]\n        label = test_file[1]\n        slice_len = self.chonk_len * int(len(test_frames)/self.chonk_len)\n\n\n        for index in range(len(test_frames[:slice_len])):\n            frame_counter += 1\n            new_frames.append(test_frames[index].flatten())\n            if frame_counter == self.chonk_len:\n                frame_counter = 0\n                blocks.append(np.array(new_frames))\n                new_labels = new_labels + [label]\n                new_frames = []\n\n        return blocks, new_labels","metadata":{"cellId":"lx0xyi3r0ynhfjlfvypn","trusted":true},"outputs":[],"execution_count":107},{"cell_type":"code","source":"#!L\ndataset = Skeleton_Dataset(data, data_path)","metadata":{"cellId":"1k6x31a2z0ipunh91m1tsg","trusted":true},"outputs":[],"execution_count":108},{"cell_type":"code","source":"#!L\ndataset.data[:int(0.75*len(dataset.data))].shape","metadata":{"cellId":"uryetmnbtdlb775awv5mrq","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"(586, 3376)"},"metadata":{}}],"execution_count":109},{"cell_type":"code","source":"#!L\nlabel = dataset.labels.values[41]\nlabel","metadata":{"cellId":"jg8yxhudlxc41qvxi4n67s","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"2"},"metadata":{}}],"execution_count":110},{"cell_type":"code","source":"#!L\nskel, lab = dataset.__getitem__(41)","metadata":{"cellId":"9rppob1gk9r3lwac6fadc2","trusted":true},"outputs":[],"execution_count":111},{"cell_type":"code","source":"#!L\nlab","metadata":{"cellId":"gac1oz3y1rror5cvykncl","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"2"},"metadata":{}}],"execution_count":112},{"cell_type":"code","source":"#!L\nskel.shape","metadata":{"cellId":"u4vufy7zyyf3qa4fb853vc","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"(45, 75)"},"metadata":{}}],"execution_count":113},{"cell_type":"code","source":"#!L\naction_classes = [\n    \"drink water\",\n    \"eat meal/snack\",\n    \"brushing teeth\",\n    \"brushing hair\",\n    \"drop\",\n    \"pickup\",\n    \"throw\",\n    \"sitting down\",\n    \"standing up (from sitting position)\",\n    \"clapping\",\n    \"reading\",\n    \"writing\",\n    \"tear up paper\",\n    \"wear jacket\",\n    \"take off jacket\",\n    \"wear a shoe\",\n    \"take off a shoe\",\n    \"wear on glasses\",\n    \"take off glasses\",\n    \"put on a hat/cap\",\n    \"take off a hat/cap\",\n    \"cheer up\",\n    \"hand waving\",\n    \"kicking something\",\n    \"reach into pocket\",\n    \"hopping (one foot jumping)\",\n    \"jump up\",\n    \"make a phone call/answer phone\",\n    \"playing with phone/tablet\",\n    \"typing on a keyboard\",\n    \"pointing to something with finger\",\n    \"taking a selfie\",\n    \"check time (from watch)\",\n    \"rub two hands together\",\n    \"nod head/bow\",\n    \"shake head\",\n    \"wipe face\",\n    \"salute\",\n    \"put the palms together\",\n    \"cross hands in front (say stop)\",\n    \"sneeze/cough\",\n    \"staggering\",\n    \"falling\",\n    \"touch head (headache)\",\n    \"touch chest (stomachache/heart pain)\",\n    \"touch back (backache)\",\n    \"touch neck (neckache)\",\n    \"nausea or vomiting condition\",\n    \"use a fan (with hand or paper)/feeling warm\",\n    \"punching/slapping other person\",\n    \"kicking other person\",\n    \"pushing other person\",\n    \"pat on back of other person\",\n    \"point finger at the other person\",\n    \"hugging other person\",\n    \"giving something to other person\",\n    \"touch other person's pocket\",\n    \"handshaking\",\n    \"walking towards each other\",\n    \"walking apart from each other\",\n    \"put on headphone\",\n    \"take off headphone\",\n    \"shoot at the basket\",\n    \"bounce ball\",\n    \"tennis bat swing\",\n    \"juggling table tennis balls\",\n    \"hush (quite)\",\n    \"flick hair\",\n    \"thumb up\",\n    \"thumb down\",\n    \"make ok sign\",\n    \"make victory sign\",\n    \"staple book\",\n    \"counting money\",\n    \"cutting nails\",\n    \"cutting paper (using scissors)\",\n    \"snapping fingers\",\n    \"open bottle\",\n    \"sniff (smell)\",\n    \"squat down\",\n    \"toss a coin\",\n    \"fold paper\",\n    \"ball up paper\",\n    \"play magic cube\",\n    \"apply cream on face\",\n    \"apply cream on hand back\",\n    \"put on bag\",\n    \"take off bag\",\n    \"put something into a bag\",\n    \"take something out of a bag\",\n    \"open a box\",\n    \"move heavy objects\",\n    \"shake fist\",\n    \"throw up cap/hat\",\n    \"hands up (both hands)\",\n    \"cross arms\",\n    \"arm circles\",\n    \"arm swings\",\n    \"running on the spot\",\n    \"butt kicks (kick backward)\",\n    \"cross toe touch\",\n    \"side kick\",\n    \"yawn\",\n    \"stretch oneself\",\n    \"blow nose\",\n    \"hit other person with something\",\n    \"wield knife towards other person\",\n    \"knock over other person (hit with body)\",\n    \"grab other person’s stuff\",\n    \"shoot at other person with a gun\",\n    \"step on foot\",\n    \"high-five\",\n    \"cheers and drink\",\n    \"carry something with other person\",\n    \"take a photo of other person\",\n    \"follow other person\",\n    \"whisper in other person’s ear\",\n    \"exchange things with other person\",\n    \"support somebody with hand\",\n    \"finger-guessing game (playing rock-paper-scissors)\"\n]\nLABELS = {0: \"cheer up\", 1: \"jump up\", 2:  \"hand waving\", 3: \"sitting down\", 4: \"clapping\"}\nclasses = [8, 10, 22, 23, 27, 21]    \nLABELS = {i: action_classes[classes[i]] for i in range(6)}\nLABELS","metadata":{"cellId":"p2rcgnudvhm9spvaddeca5","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"{0: 'standing up (from sitting position)',\n 1: 'reading',\n 2: 'hand waving',\n 3: 'kicking something',\n 4: 'make a phone call/answer phone',\n 5: 'cheer up'}"},"metadata":{}}],"execution_count":115},{"cell_type":"code","source":"#!L\nLABELS[lab]","metadata":{"cellId":"vxpne3m3e4qptvfnx2t8a","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"'hand waving'"},"metadata":{}}],"execution_count":116},{"cell_type":"markdown","source":"## Задание 2","metadata":{"cellId":"o0chk4pkzdr9xcsp2u56b"}},{"cell_type":"markdown","source":"Построить график зависимости от количества кадров в последовательности (уменьшать и увеличивать)","metadata":{"cellId":"fv2l0gnrpor0g2aqdv0uosf"}},{"cell_type":"code","source":"#!L\nclass LSTM_net(nn.Module):\n    def __init__(self,input_dim, hidden_dim, output_dim, layer_num):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.lstm = torch.nn.LSTM(input_dim, hidden_dim,layer_num,batch_first=True)\n        self.dr = torch.nn.Dropout2d(0.1)\n        self.fc = torch.nn.Linear(hidden_dim,output_dim)\n        \n        \n    def forward(self,inputs):\n        x = inputs\n        lstm_out,(hn,cn) = self.lstm(x)\n        out = self.fc(lstm_out[:,-1,:])\n        return out","metadata":{"cellId":"gbkndxkpuss1jpbq8of9p1","trusted":true},"outputs":[],"execution_count":119},{"cell_type":"code","source":"#!L\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","metadata":{"cellId":"dx6uvaif0np0xrp22vft89","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":120},{"cell_type":"code","source":"#!L\nn_hidden = 128\nn_joints = 25*3\nn_categories = len(LABELS)\nn_layer = 2\nrnn = LSTM_net(n_joints,n_hidden,n_categories,n_layer)\nrnn.to(device)","metadata":{"cellId":"a0naavvtm1oq85m5ec5di7","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"LSTM_net(\n  (lstm): LSTM(75, 128, num_layers=2, batch_first=True)\n  (dr): Dropout2d(p=0.1, inplace=False)\n  (fc): Linear(in_features=128, out_features=6, bias=True)\n)"},"metadata":{}}],"execution_count":121},{"cell_type":"code","source":"#!L\ndef categoryFromOutput(output):\n    top_n, top_i = output.topk(1)\n    category_i = top_i[0].item()\n#     print(output.topk(5))\n    return LABELS[category_i], category_i\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)","metadata":{"cellId":"03qfp68mm3om0gho5z7mgkfh","trusted":true},"outputs":[],"execution_count":122},{"cell_type":"markdown","source":"## 45 кадров","metadata":{"cellId":"cgzawlz59hpcae65azerek"}},{"cell_type":"code","source":"#!L\nsplit_train_part = 0.75\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, \n                                                            [int(split_train_part*dataset.data.shape[0]), \n                                                             dataset.data.shape[0] - int(split_train_part*dataset.data.shape[0])])\ntrain_loader = DataLoader(train_dataset, batch_size = 16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size = 1, shuffle=False)","metadata":{"cellId":"qpgzucqc6iswhaffq9cbbm","trusted":true},"outputs":[],"execution_count":123},{"cell_type":"code","source":"#!L\nfrom torch import optim\nimport time\nimport math\n\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.0007\noptimizer = optim.SGD(rnn.parameters(), lr=learning_rate, momentum=0.9)\n\nall_losses = []\nstart = time.time()\ncounter = 0\nfor epoch in range(200):  \n    current_loss = 0\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        \n        inputs, labels = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n    \n        output = rnn(inputs.float())\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step() \n\n\n        current_loss += loss.item()\n        category = LABELS[int(labels[0])]\n\n        if counter % 500 == 0:\n            guess, guess_i = categoryFromOutput(output)\n            correct = '✓' if guess == category else '✗ (%s)' % category\n            print('epoch : %d iter : %d (%s) %.4f  / %s %s' % (epoch, i, timeSince(start), loss, guess, correct))\n\n        \n        counter = counter + 1\n    if counter % 100 == 0:\n        all_losses.append(current_loss / 25)\n        current_loss = 0","metadata":{"cellId":"t8nw8lsw1bkljjv9ksnqpc","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"epoch : 0 iter : 0 (0m 0s) 1.7793  / standing up (from sitting position) ✗ (reading)\nepoch : 13 iter : 19 (0m 20s) 1.7650  / hand waving ✗ (make a phone call/answer phone)\nepoch : 27 iter : 1 (0m 41s) 1.5809  / standing up (from sitting position) ✗ (make a phone call/answer phone)\nepoch : 40 iter : 20 (1m 1s) 1.7983  / reading ✗ (make a phone call/answer phone)\nepoch : 54 iter : 2 (1m 22s) 1.3202  / reading ✓\nepoch : 67 iter : 21 (1m 42s) 1.2090  / standing up (from sitting position) ✓\nepoch : 81 iter : 3 (2m 3s) 0.9431  / kicking something ✓\nepoch : 94 iter : 22 (2m 24s) 0.9488  / make a phone call/answer phone ✗ (cheer up)\nepoch : 108 iter : 4 (2m 44s) 1.0420  / reading ✗ (cheer up)\nepoch : 121 iter : 23 (3m 4s) 1.3536  / make a phone call/answer phone ✓\nepoch : 135 iter : 5 (3m 25s) 0.6645  / reading ✓\nepoch : 148 iter : 24 (3m 46s) 1.1627  / make a phone call/answer phone ✗ (reading)\nepoch : 162 iter : 6 (4m 6s) 0.4204  / cheer up ✗ (reading)\nepoch : 175 iter : 25 (4m 27s) 0.6787  / hand waving ✗ (reading)\nepoch : 189 iter : 7 (4m 47s) 0.7665  / standing up (from sitting position) ✓\n"}],"execution_count":124},{"cell_type":"markdown","source":"## 35 кадров","metadata":{"cellId":"tqv07xbjixesa4fdesojte"}},{"cell_type":"code","source":"#!L\ndata_path = \"nturgb+d_skeletons/\"\nbroken_files_path = \"NTU_RGBD_samples_with_missing_skeletons.txt\"\n\ndata = Croped_Raw_Data(data_path, broken_files_path)\ndataset_c35 = Skeleton_Dataset(data, data_path, chonk_len=35)","metadata":{"cellId":"w3hjoiqm839gws2fzxnt3a","trusted":true},"outputs":[],"execution_count":126},{"cell_type":"code","source":"#!L\ndataset_c35.data.shape","metadata":{"cellId":"p81f09t0ts9ri1np0s385","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"(905, 2626)"},"metadata":{}}],"execution_count":131},{"cell_type":"code","source":"#!L\nskel, lab = dataset_c35.__getitem__(41)","metadata":{"cellId":"ix92ward85f5jemp40ame9","trusted":true},"outputs":[],"execution_count":132},{"cell_type":"code","source":"#!L\nskel.shape","metadata":{"cellId":"bb9m7ielwjiufz44w4395k","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"(35, 75)"},"metadata":{}}],"execution_count":133},{"cell_type":"code","source":"#!L\nlab","metadata":{"cellId":"2mti7yqd61mp5t4onr3muf","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"5"},"metadata":{}}],"execution_count":134},{"cell_type":"code","source":"#!L\nsplit_train_part = 0.75\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset_c35, \n                                                            [int(split_train_part*dataset_c35.data.shape[0]), \n                                                             dataset_c35.data.shape[0] - int(split_train_part*dataset_c35.data.shape[0])], \n                                                            generator=torch.Generator().manual_seed(42))\ntrain_loader = DataLoader(train_dataset, batch_size = 16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size = 1, shuffle=False)","metadata":{"cellId":"94ca7as24zkzw7k8q805h","trusted":true},"outputs":[],"execution_count":135},{"cell_type":"code","source":"#!L\nn_hidden = 128\nn_joints = 25*3\nn_categories = len(LABELS)\nn_layer = 2\nrnn = LSTM_net(n_joints,n_hidden,n_categories,n_layer)\nrnn.to(device)","metadata":{"cellId":"gtro87hgkvh5rkpmjj43pr","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"LSTM_net(\n  (lstm): LSTM(75, 128, num_layers=2, batch_first=True)\n  (dr): Dropout2d(p=0.1, inplace=False)\n  (fc): Linear(in_features=128, out_features=6, bias=True)\n)"},"metadata":{}}],"execution_count":137},{"cell_type":"code","source":"#!L\nfrom torch import optim\nimport time\nimport math\n\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.0007\noptimizer = optim.SGD(rnn.parameters(), lr=learning_rate, momentum=0.9)\n\nall_losses = []\nstart = time.time()\ncounter = 0\nfor epoch in range(200):  \n    current_loss = 0\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        \n        inputs, labels = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n    \n        output = rnn(inputs.float())\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step() \n\n\n        current_loss += loss.item()\n        category = LABELS[int(labels[0])]\n\n        if counter % 500 == 0:\n            guess, guess_i = categoryFromOutput(output)\n            correct = '✓' if guess == category else '✗ (%s)' % category\n            print('epoch : %d iter : %d (%s) %.4f  / %s %s' % (epoch, i, timeSince(start), loss, guess, correct))\n\n        \n        counter = counter + 1\n    if counter % 100 == 0:\n        all_losses.append(current_loss / 25)\n        current_loss = 0","metadata":{"cellId":"gtziqrl7qtfa6k28ke17u","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"epoch : 0 iter : 0 (0m 0s) 1.8289  / standing up (from sitting position) ✓\nepoch : 11 iter : 27 (0m 17s) 1.7605  / hand waving ✗ (cheer up)\nepoch : 23 iter : 11 (0m 34s) 1.5320  / hand waving ✗ (cheer up)\nepoch : 34 iter : 38 (0m 51s) 1.4354  / standing up (from sitting position) ✗ (cheer up)\nepoch : 46 iter : 22 (1m 8s) 1.6923  / hand waving ✗ (cheer up)\nepoch : 58 iter : 6 (1m 25s) 1.2300  / hand waving ✗ (kicking something)\nepoch : 69 iter : 33 (1m 42s) 1.1184  / kicking something ✓\nepoch : 81 iter : 17 (1m 59s) 1.0343  / hand waving ✗ (kicking something)\nepoch : 93 iter : 1 (2m 16s) 1.1912  / kicking something ✗ (hand waving)\nepoch : 104 iter : 28 (2m 33s) 0.8668  / reading ✓\nepoch : 116 iter : 12 (2m 50s) 0.7947  / kicking something ✗ (hand waving)\nepoch : 127 iter : 39 (3m 7s) 1.0142  / hand waving ✗ (make a phone call/answer phone)\nepoch : 139 iter : 23 (3m 24s) 1.1213  / reading ✗ (standing up (from sitting position))\nepoch : 151 iter : 7 (3m 41s) 0.8545  / make a phone call/answer phone ✓\nepoch : 162 iter : 34 (3m 58s) 0.8356  / kicking something ✓\nepoch : 174 iter : 18 (4m 15s) 0.7865  / make a phone call/answer phone ✗ (reading)\nepoch : 186 iter : 2 (4m 32s) 0.9309  / reading ✓\nepoch : 197 iter : 29 (4m 49s) 0.8900  / standing up (from sitting position) ✓\n"}],"execution_count":138},{"cell_type":"markdown","source":"## Перебор вариантов: 30 - 45 кадров, 1-3 LSTM слоёв","metadata":{"cellId":"nrlxqceurzp2zi0qnf6s"}},{"cell_type":"code","source":"#!L\nimport math\n\nclass HyperParSearchDec:\n    \n    def __init__(self):\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.net = None\n        self.summary_data = pd.DataFrame(data=[[0, 0, 0, 0]], \n                            columns=['layer', 'chonk', 'loss', 'accuracy'])\n        self.chonk = np.arange(30, 46, 5)\n        self.layer = np.arange(3) + 1\n        self.epochs = 200\n        self.data_path = \"nturgb+d_skeletons/\"\n        broken_files_path = \"NTU_RGBD_samples_with_missing_skeletons.txt\"\n\n        self.data = Croped_Raw_Data(self.data_path, broken_files_path)\n        self.prev_chonk = 0\n        self.train_ld = None\n\n    def net_sample(self, chonk):\n        criterion = nn.CrossEntropyLoss()\n        learning_rate = 0.0007\n        optimizer = optim.SGD(self.net.parameters(), lr=learning_rate, momentum=0.9)\n\n        ep_loss_list = []\n        ep_acc_list = []\n        start = time.time()\n        counter = 0\n        \n        split_train_part = 0.75\n        if self.prev_chonk != chonk:\n            dataset = Skeleton_Dataset(self.data, self.data_path, chonk_len=chonk)\n            train_dataset, test_dataset = torch.utils.data.random_split(dataset, \n                                                                        [int(split_train_part*dataset.data.shape[0]), \n                                                                         dataset.data.shape[0] - int(split_train_part*dataset.data.shape[0])], \n                                                                        generator=torch.Generator().manual_seed(42))\n            self.train_ld = DataLoader(train_dataset, batch_size = 16, shuffle=True)\n#             test_ld = DataLoader(test_dataset, batch_size = 1, shuffle=False)\n            self.prev_chonk = chonk\n        \n        for epoch in range(self.epochs):\n            loss_list = []\n            acc_list = []\n            \n            for i, data in enumerate(self.train_ld, 0):\n                inputs, labels = data[0].to(device), data[1].to(device)\n\n                # обнуляем градиент\n                optimizer.zero_grad()\n\n                outputs = self.net(inputs.float())\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                # print statistics\n                category = LABELS[int(labels[0])]\n\n#                 if counter % 300 == 0:\n#                     guess, guess_i = self.categoryFromOutput(output)\n#                     correct = '✓' if guess == category else '✗ (%s)' % category\n#                     print('\\t\\t\\tepoch : %d iter : %d (%s) %.4f  / %s %s' % (epoch, i, self.timeSince(start), loss, guess, correct))\n#                 counter = counter + 1\n\n                # выводим статистику о процессе обучения\n                loss_list.append(loss.item())\n\n                # Отслеживание точности\n                total = labels.size(0)\n                _, predicted = torch.max(outputs.data, 1)\n                correct = (predicted == labels).sum().item()\n                acc_list.append(correct / total)\n                \n            ep_loss_list.append(sum(loss_list) / len(loss_list))\n            ep_acc_list.append(sum(acc_list) / len(acc_list))\n            counter = counter + 1\n            if counter % 50 == 0:\n                print(f\"\\t\\t\\tЭпоха: {epoch+1}\\tloss: {ep_loss_list[-1]:.6f}\\taccuracy: {ep_acc_list[-1]*100:.3f}%\")\n        return ep_loss_list, ep_acc_list\n        \n    def grid_search(self):\n        cnt = 0\n        \n        path_dir = './net_lstm'\n        if not os.path.exists(path_dir):\n            os.mkdir(path_dir)\n        \n        for n_ch in self.chonk:\n            for n_la in self.layer:\n                print(f\"\\nМодель: слоёв lstm: {n_la}, кадров: {n_ch}\\n\")\n                n_hidden = 128\n                n_joints = 25*3\n                n_categories = len(LABELS)\n                self.net = LSTM_net(n_joints, n_hidden, n_categories, n_la).to(self.device)\n                print(\"\\tМодель state_dict: \")\n                for param in self.net.state_dict():\n                    print(\"\\t\\t\", param,\": \", self.net.state_dict()[param].size())\n                loss_t, acc_t = self.net_sample(n_ch)\n                loss = loss_t[-1]\n                acc = acc_t[-1]\n                torch.save(self.net.state_dict(), f'./net_lstm/op_net_lstm_{n_la}_chonk_{n_ch}.pth')\n\n                print(f\"\\tОшибки на train: {loss:.5f}\")\n                print(f\"\\tAccuracy на train: {acc:.5f}\")\n                self.summary_data.loc[cnt, ['layer', 'chonk', 'loss', 'accuracy']] = [n_la, n_ch, loss, acc]\n                cnt += 1\n                \n    def categoryFromOutput(self, output):\n        top_n, top_i = output.topk(1)\n        category_i = top_i[0].item()\n    #     print(output.topk(5))\n        return LABELS[category_i], category_i\n\n    def timeSince(self, since):\n        now = time.time()\n        s = now - since\n        m = math.floor(s / 60)\n        s -= m * 60\n        return '%dm %ds' % (m, s)","metadata":{"cellId":"kqyvybulrkdnvlgpobnbue","trusted":true},"outputs":[],"execution_count":168},{"cell_type":"code","source":"#!L\nsearch_top = HyperParSearchDec()","metadata":{"cellId":"ns5wv539itgvqy2d7o1hv","trusted":true},"outputs":[],"execution_count":169},{"cell_type":"code","source":"#!L\nsearch_top.grid_search()","metadata":{"cellId":"yl3hmrvqtlcqkeuqtrpns","trusted":true},"outputs":[{"name":"stdout","text":"\nМодель: слоёв lstm: 1, кадров: 30\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.064568\taccuracy: 60.772%\n\t\t\tЭпоха: 100\tloss: 0.901031\taccuracy: 65.003%\n\t\t\tЭпоха: 150\tloss: 0.937131\taccuracy: 62.920%\n\t\t\tЭпоха: 200\tloss: 0.816231\taccuracy: 68.556%\n\tОшибки на train: 0.81623\n\tAccuracy на train: 0.68556\n\nМодель: слоёв lstm: 2, кадров: 30\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t lstm.weight_ih_l1 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l1 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l1 :  torch.Size([512])\n\t\t lstm.bias_hh_l1 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.275710\taccuracy: 51.550%\n\t\t\tЭпоха: 100\tloss: 1.018779\taccuracy: 60.126%\n\t\t\tЭпоха: 150\tloss: 0.846537\taccuracy: 66.457%\n\t\t\tЭпоха: 200\tloss: 0.731337\taccuracy: 71.754%\n\tОшибки на train: 0.73134\n\tAccuracy на train: 0.71754\n\nМодель: слоёв lstm: 3, кадров: 30\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t lstm.weight_ih_l1 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l1 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l1 :  torch.Size([512])\n\t\t lstm.bias_hh_l1 :  torch.Size([512])\n\t\t lstm.weight_ih_l2 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l2 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l2 :  torch.Size([512])\n\t\t lstm.bias_hh_l2 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.783156\taccuracy: 29.231%\n\t\t\tЭпоха: 100\tloss: 1.446792\taccuracy: 38.098%\n\t\t\tЭпоха: 150\tloss: 1.122974\taccuracy: 56.815%\n\t\t\tЭпоха: 200\tloss: 0.968790\taccuracy: 61.902%\n\tОшибки на train: 0.96879\n\tAccuracy на train: 0.61902\n\nМодель: слоёв lstm: 1, кадров: 35\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.129961\taccuracy: 57.074%\n\t\t\tЭпоха: 100\tloss: 1.113670\taccuracy: 61.095%\n\t\t\tЭпоха: 150\tloss: 0.873206\taccuracy: 65.165%\n\t\t\tЭпоха: 200\tloss: 0.762770\taccuracy: 69.525%\n\tОшибки на train: 0.76277\n\tAccuracy на train: 0.69525\n\nМодель: слоёв lstm: 2, кадров: 35\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t lstm.weight_ih_l1 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l1 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l1 :  torch.Size([512])\n\t\t lstm.bias_hh_l1 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.379737\taccuracy: 45.882%\n\t\t\tЭпоха: 100\tloss: 0.996648\taccuracy: 61.773%\n\t\t\tЭпоха: 150\tloss: 0.906154\taccuracy: 66.037%\n\t\t\tЭпоха: 200\tloss: 0.810456\taccuracy: 70.543%\n\tОшибки на train: 0.81046\n\tAccuracy на train: 0.70543\n\nМодель: слоёв lstm: 3, кадров: 35\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t lstm.weight_ih_l1 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l1 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l1 :  torch.Size([512])\n\t\t lstm.bias_hh_l1 :  torch.Size([512])\n\t\t lstm.weight_ih_l2 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l2 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l2 :  torch.Size([512])\n\t\t lstm.bias_hh_l2 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.685499\taccuracy: 33.091%\n\t\t\tЭпоха: 100\tloss: 1.311621\taccuracy: 42.490%\n\t\t\tЭпоха: 150\tloss: 1.098125\taccuracy: 56.589%\n\t\t\tЭпоха: 200\tloss: 0.973327\taccuracy: 60.659%\n\tОшибки на train: 0.97333\n\tAccuracy на train: 0.60659\n\nМодель: слоёв lstm: 1, кадров: 40\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.152849\taccuracy: 57.300%\n\t\t\tЭпоха: 100\tloss: 1.196494\taccuracy: 56.690%\n\t\t\tЭпоха: 150\tloss: 1.152923\taccuracy: 60.688%\n\t\t\tЭпоха: 200\tloss: 1.025213\taccuracy: 62.940%\n\tОшибки на train: 1.02521\n\tAccuracy на train: 0.62940\n\nМодель: слоёв lstm: 2, кадров: 40\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t lstm.weight_ih_l1 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l1 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l1 :  torch.Size([512])\n\t\t lstm.bias_hh_l1 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.379377\taccuracy: 46.697%\n\t\t\tЭпоха: 100\tloss: 0.941366\taccuracy: 66.294%\n\t\t\tЭпоха: 150\tloss: 0.865759\taccuracy: 68.513%\n\t\t\tЭпоха: 200\tloss: 0.681315\taccuracy: 74.221%\n\tОшибки на train: 0.68132\n\tAccuracy на train: 0.74221\n\nМодель: слоёв lstm: 3, кадров: 40\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t lstm.weight_ih_l1 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l1 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l1 :  torch.Size([512])\n\t\t lstm.bias_hh_l1 :  torch.Size([512])\n\t\t lstm.weight_ih_l2 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l2 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l2 :  torch.Size([512])\n\t\t lstm.bias_hh_l2 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.761532\taccuracy: 32.148%\n\t\t\tЭпоха: 100\tloss: 1.384651\taccuracy: 39.262%\n\t\t\tЭпоха: 150\tloss: 1.203998\taccuracy: 52.304%\n\t\t\tЭпоха: 200\tloss: 1.048996\taccuracy: 60.196%\n\tОшибки на train: 1.04900\n\tAccuracy на train: 0.60196\n\nМодель: слоёв lstm: 1, кадров: 45\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.292957\taccuracy: 48.209%\n\t\t\tЭпоха: 100\tloss: 1.172308\taccuracy: 56.824%\n\t\t\tЭпоха: 150\tloss: 0.961708\taccuracy: 61.757%\n\t\t\tЭпоха: 200\tloss: 0.897754\taccuracy: 66.824%\n\tОшибки на train: 0.89775\n\tAccuracy на train: 0.66824\n\nМодель: слоёв lstm: 2, кадров: 45\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t lstm.weight_ih_l1 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l1 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l1 :  torch.Size([512])\n\t\t lstm.bias_hh_l1 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.379707\taccuracy: 47.162%\n\t\t\tЭпоха: 100\tloss: 1.075468\taccuracy: 59.797%\n\t\t\tЭпоха: 150\tloss: 0.697733\taccuracy: 75.811%\n\t\t\tЭпоха: 200\tloss: 0.866848\taccuracy: 68.716%\n\tОшибки на train: 0.86685\n\tAccuracy на train: 0.68716\n\nМодель: слоёв lstm: 3, кадров: 45\n\n\tМодель state_dict: \n\t\t lstm.weight_ih_l0 :  torch.Size([512, 75])\n\t\t lstm.weight_hh_l0 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l0 :  torch.Size([512])\n\t\t lstm.bias_hh_l0 :  torch.Size([512])\n\t\t lstm.weight_ih_l1 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l1 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l1 :  torch.Size([512])\n\t\t lstm.bias_hh_l1 :  torch.Size([512])\n\t\t lstm.weight_ih_l2 :  torch.Size([512, 128])\n\t\t lstm.weight_hh_l2 :  torch.Size([512, 128])\n\t\t lstm.bias_ih_l2 :  torch.Size([512])\n\t\t lstm.bias_hh_l2 :  torch.Size([512])\n\t\t fc.weight :  torch.Size([6, 128])\n\t\t fc.bias :  torch.Size([6])\n\t\t\tЭпоха: 50\tloss: 1.772261\taccuracy: 18.851%\n\t\t\tЭпоха: 100\tloss: 1.400819\taccuracy: 43.176%\n\t\t\tЭпоха: 150\tloss: 1.183418\taccuracy: 49.797%\n\t\t\tЭпоха: 200\tloss: 0.899629\taccuracy: 66.791%\n\tОшибки на train: 0.89963\n\tAccuracy на train: 0.66791\n","output_type":"stream"}],"execution_count":170},{"cell_type":"code","source":"#!L\nsearch_top.summary_data","metadata":{"cellId":"muylfnf1cqqfukkyfmdjqe","trusted":true},"outputs":[{"execution_count":171,"output_type":"execute_result","data":{"text/plain":"    layer  chonk      loss  accuracy\n0     1.0   30.0  0.816231  0.685562\n1     2.0   30.0  0.731337  0.717539\n2     3.0   30.0  0.968790  0.619025\n3     1.0   35.0  0.762770  0.695252\n4     2.0   35.0  0.810456  0.705426\n5     3.0   35.0  0.973327  0.606589\n6     1.0   40.0  1.025213  0.629404\n7     2.0   40.0  0.681315  0.742209\n8     3.0   40.0  1.048996  0.601965\n9     1.0   45.0  0.897754  0.668243\n10    2.0   45.0  0.866848  0.687162\n11    3.0   45.0  0.899629  0.667905","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>layer</th>\n      <th>chonk</th>\n      <th>loss</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>30.0</td>\n      <td>0.816231</td>\n      <td>0.685562</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>30.0</td>\n      <td>0.731337</td>\n      <td>0.717539</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>30.0</td>\n      <td>0.968790</td>\n      <td>0.619025</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>35.0</td>\n      <td>0.762770</td>\n      <td>0.695252</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>35.0</td>\n      <td>0.810456</td>\n      <td>0.705426</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3.0</td>\n      <td>35.0</td>\n      <td>0.973327</td>\n      <td>0.606589</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.0</td>\n      <td>40.0</td>\n      <td>1.025213</td>\n      <td>0.629404</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.0</td>\n      <td>40.0</td>\n      <td>0.681315</td>\n      <td>0.742209</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3.0</td>\n      <td>40.0</td>\n      <td>1.048996</td>\n      <td>0.601965</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.0</td>\n      <td>45.0</td>\n      <td>0.897754</td>\n      <td>0.668243</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2.0</td>\n      <td>45.0</td>\n      <td>0.866848</td>\n      <td>0.687162</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3.0</td>\n      <td>45.0</td>\n      <td>0.899629</td>\n      <td>0.667905</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":171},{"cell_type":"code","source":"#!L\nsearch_top.summary_data.sort_values('accuracy', ascending=False)","metadata":{"cellId":"619rgcb83gyuqmtop5ras","trusted":true},"outputs":[{"execution_count":172,"output_type":"execute_result","data":{"text/plain":"    layer  chonk      loss  accuracy\n7     2.0   40.0  0.681315  0.742209\n1     2.0   30.0  0.731337  0.717539\n4     2.0   35.0  0.810456  0.705426\n3     1.0   35.0  0.762770  0.695252\n10    2.0   45.0  0.866848  0.687162\n0     1.0   30.0  0.816231  0.685562\n9     1.0   45.0  0.897754  0.668243\n11    3.0   45.0  0.899629  0.667905\n6     1.0   40.0  1.025213  0.629404\n2     3.0   30.0  0.968790  0.619025\n5     3.0   35.0  0.973327  0.606589\n8     3.0   40.0  1.048996  0.601965","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>layer</th>\n      <th>chonk</th>\n      <th>loss</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>2.0</td>\n      <td>40.0</td>\n      <td>0.681315</td>\n      <td>0.742209</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>30.0</td>\n      <td>0.731337</td>\n      <td>0.717539</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>35.0</td>\n      <td>0.810456</td>\n      <td>0.705426</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>35.0</td>\n      <td>0.762770</td>\n      <td>0.695252</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2.0</td>\n      <td>45.0</td>\n      <td>0.866848</td>\n      <td>0.687162</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>30.0</td>\n      <td>0.816231</td>\n      <td>0.685562</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.0</td>\n      <td>45.0</td>\n      <td>0.897754</td>\n      <td>0.668243</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3.0</td>\n      <td>45.0</td>\n      <td>0.899629</td>\n      <td>0.667905</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.0</td>\n      <td>40.0</td>\n      <td>1.025213</td>\n      <td>0.629404</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>30.0</td>\n      <td>0.968790</td>\n      <td>0.619025</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3.0</td>\n      <td>35.0</td>\n      <td>0.973327</td>\n      <td>0.606589</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3.0</td>\n      <td>40.0</td>\n      <td>1.048996</td>\n      <td>0.601965</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":172},{"cell_type":"code","source":"#!L\nsearch_top.summary_data.to_csv(\"summary_data.csv\", index=False)","metadata":{"cellId":"4qb3uqa8kc87igvkvej5lq","trusted":true},"outputs":[],"execution_count":173},{"cell_type":"code","source":"sum_dat = pd.read_csv(\"summary_data.csv\")\nsum_dat","metadata":{"cellId":"x7wvkqa8b9fwl7omibsu6g","trusted":true},"outputs":[{"execution_count":174,"output_type":"execute_result","data":{"text/plain":"    layer  chonk      loss  accuracy\n0     1.0   30.0  0.816231  0.685562\n1     2.0   30.0  0.731337  0.717539\n2     3.0   30.0  0.968790  0.619025\n3     1.0   35.0  0.762770  0.695252\n4     2.0   35.0  0.810456  0.705426\n5     3.0   35.0  0.973327  0.606589\n6     1.0   40.0  1.025213  0.629404\n7     2.0   40.0  0.681315  0.742209\n8     3.0   40.0  1.048996  0.601965\n9     1.0   45.0  0.897754  0.668243\n10    2.0   45.0  0.866848  0.687162\n11    3.0   45.0  0.899629  0.667905","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>layer</th>\n      <th>chonk</th>\n      <th>loss</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>30.0</td>\n      <td>0.816231</td>\n      <td>0.685562</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>30.0</td>\n      <td>0.731337</td>\n      <td>0.717539</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>30.0</td>\n      <td>0.968790</td>\n      <td>0.619025</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>35.0</td>\n      <td>0.762770</td>\n      <td>0.695252</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>35.0</td>\n      <td>0.810456</td>\n      <td>0.705426</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3.0</td>\n      <td>35.0</td>\n      <td>0.973327</td>\n      <td>0.606589</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.0</td>\n      <td>40.0</td>\n      <td>1.025213</td>\n      <td>0.629404</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.0</td>\n      <td>40.0</td>\n      <td>0.681315</td>\n      <td>0.742209</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3.0</td>\n      <td>40.0</td>\n      <td>1.048996</td>\n      <td>0.601965</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.0</td>\n      <td>45.0</td>\n      <td>0.897754</td>\n      <td>0.668243</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2.0</td>\n      <td>45.0</td>\n      <td>0.866848</td>\n      <td>0.687162</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3.0</td>\n      <td>45.0</td>\n      <td>0.899629</td>\n      <td>0.667905</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":174},{"cell_type":"code","source":"","metadata":{"cellId":"zn7uvq5oj6ei45sk4mdum"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Задание 3","metadata":{"cellId":"dzcrid5myn7zmslve4uns"}},{"cell_type":"markdown","source":"Построить график зависимости от количества модулей LSTM","metadata":{"cellId":"4kh3lvhik2tte6286eyb38"}},{"cell_type":"code","source":"","metadata":{"cellId":"wqm8yrv0slcp23xromnvt"},"outputs":[],"execution_count":null}]}